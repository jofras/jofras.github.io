<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<script>
if (localStorage.getItem("theme") === "dark") {
document.documentElement.setAttribute("data-theme", "dark");
    }
</script>
<title>Lost in Transcription</title>
  <script type="module" crossorigin src="/assets/main-C7uEwHCZ.js"></script>
  <link rel="stylesheet" crossorigin href="/assets/main-renB4jpI.css">
</head>
<body>
<site-navbar></site-navbar>
<main class="container">
<article class="article">
<h1>Lost in Transcription</h1>
<p class="subtitle">
    Investigating how speech-to-text errors corrupt language models trained on podcast transcripts, and how different media construct meaning.
</p>
<img src="/images/inlp.jpg" alt="Data preprocessing pipeline" />
<p>
    This project started as an investigation into gender bias in podcasts but turned into something else entirely. I wanted to compare spoken language (podcasts) against written text (news) using a massive dataset from 2020. But deep in preprocessing, I hit a wall: the dataset was broken. The signal of human conversation was being drowned out by the noise of the transcription AI, which was hallucinating text that didn't exist.
</p>
<p>
    Instead of ignoring it, I pivoted. I built a custom pipeline to detect and scrub these errors from over 169 million sentences. Once the data was clean, I trained machine learning models to map the "semantic reality" of podcasts, revealing how they frame crises and gender differently than traditional news.
</p>
<p>
    This is a design-oriented description of the project. For the full scientific paper, you can download the thesis <a href="/thesis.pdf">here</a> or check out the code on <a href="https://github.com/jofras/bachelors-thesis">GitHub</a>.
</p>

<h2>Motivation</h2>
<p>
My interest started after reading Brian Christian's <em>The Alignment Problem</em>. I was fascinated by latent bias in AI systems, like the COMPAS recidivism algorithm or Joy Buolamwini's work on facial recognition. I wanted to do something similar for my thesis: dig into the messy, human side of Natural Language Processing.
</p>
<p>
I joined Rycolab at ETH to look at gender bias in the SPoRC dataset, a massive collection of podcast transcripts from 2020. They gave me a lot of freedom, which was lucky, because the scale and subtle contamination of the corpus forced me to shift focus almost immediately.
</p>
<p>
I was working on a local Mac with 8GB of RAM, trying to process 50GB of raw transcription JSONs, full of metadata. I had to optimize drastically just to get work done. About two or three months into preprocessing, scrolling through what I thought was finally clean data, I noticed something weird. The model wasn't just transcribing. It was looping.
</p>
<img src="/images/run-length-distro.jpg" alt="Distribution of hallucination loop lengths" />
<p>
I found a specific file where the transcript didn't match the audio. The podcast was an emotional episode about police brutality, discussing George Floyd and Ahmaud Arbery. But in the transcript, huge chunks of that conversation were gone, overwritten by the AI repeating a random phrase hundreds of times.
</p>
<p>
My first thought was: Is this censorship? It looked like the model was erasing sensitive political speech. I dug deeper and realized it was actually a failure mode of the Whisper ASR model. It glitches when it encounters silence or background noise.
</p>
<p>
I could have ignored it. I browsed Google Scholar; other researchers had used this dataset without noticing. But I couldn't unsee it. I realized I couldn't analyze the society in the data until I fixed the instrument collecting it.
</p>

<h2>Design</h2>
<p>
This section is about the systems I designed to handle the mess. I needed a way to process massive amounts of data without crashing my machine, and a way to ask complex questions about that data without waiting days for answers.
</p>

<h3>The Preprocessing Pipeline</h3>
<p>
I built a parallel processing pipeline that treated data cleaning like an assembly line. It takes raw, messy JSONs, strips the metadata, cleans the text using extensive regex rules (things like non-speech audio markers), and merges fragmented files back into coherent transcripts.
</p>
<img src="/images/pipeline-diagram.jpg" alt="Complete preprocessing pipeline architecture" />
<p>
This is the architecture I eventually ran on ETH's High-Performance Computing cluster. It modularizes the cleaning steps so I could iterate on specific parts, like fixing how the AI handles possessives or profanity, without re-running the whole thing.
</p>

<h3>The Database and "The Fingerprint"</h3>
<p>
The hardest part was catching the hallucinations. A human might repeat "No, no, no," but the AI would repeat a sentence 500 times. To catch this, I needed to compare millions of sentences against each other. Doing this with raw text strings is incredibly slow.
</p>
<p>
My solution was hashing. I used the xxHash algorithm to turn every sentence into a fixed-length cryptographic fingerprint. This allowed me to index the entire corpus in a PostgreSQL database and ask questions like "Where does this exact sentence appear 50 times in a row?" almost instantly.
</p>
<img src="/images/database-schema.jpg" alt="Database schema showing hash-based indexing" />
<p>
I designed this schema backwards from the questions I knew I'd need to ask. It maps every unique sentence hash to its location, allowing me to spot runs of identical hashes.
</p>

<h2>Results</h2>
<p>
Once I had the data clean, I finally got to the analysis. My goal here was to make the black box of the model transparent, to visualize what was actually happening inside the embeddings.
</p>

<h3>The Elbow of Hallucination</h3>
<p>
I needed a mathematical proof for where "human repetition" ends and "AI glitch" begins. I ran tests on all sorts of metrics, eventually stumbling upon what I dubbed the "uniqueness ratio": for each sentence loop, I compared its content to all other sentences appearing in a loop of the same length. 
</p>
<img src="/images/uniqueness-ratio.jpg" alt="Uniqueness ratio showing the transition point at run length 4" />
<p>
I call this the elbow. At a run length of 2 or 3, repeated phrases are common (humans saying "Yeah, yeah"). But past 4 repetitions, the graph shoots up, meaning each loop becomes increasingly unique. This visual proved that anything looping 4+ times was almost certainly a machine error, allowing me to filter the data safely.
</p>

<h3>The Pancake Geometry</h3>
<p>
When I trained the GloVe models, the metrics were confusing. The vectors were pointing in all directions (high isotropy), which usually means they're capturing complex relationships. But they also had very low variance, meaning they were "flat."
</p>
<p>
To explain this, I created the pancake analogy:
</p>
<img src="/images/project1.jpg" alt="3D visualization of isotropic but low-variance vector space" />
<p>
Imagine a pancake floating in a sphere. The data points are spread out all over the pancake (high isotropy), but they don't reach up or down into the rest of the sphere (low variance). This visualization helps explain why the model seemed both rich and limited at the same time.
</p>

<h3>Divergent Semantic Realities</h3>
<p>
I compared how podcasts and news "think" about the same concepts. I used Principal Component Analysis to find the main themes in the data. The results showed that the medium changes the message.
</p>
<img src="/images/pca-table1.jpg" alt="Comparison of principal components between podcasts and news" />
<p>
This table visualizes the semantic reality of 2020. In news, COVID-19 is framed through institutions: hospitalizations, guidelines, telehealth. In podcasts, the exact same crisis is framed through the body: proteins, nutrients, inflammation. Same events, completely different worlds.
</p>
<p class="note">
<strong>Note:</strong> The podcast corpus was more topically diverse than the news corpus (which was exclusively news articles), which may have influenced these results. The finding is less about "podcasts vs. news" and more about "diverse conversational media vs. specialized written media."
</p>

<h3>Visualizing Bias</h3>
<p>
I closed the loop on my original motivation by mapping gender bias.
</p>
<img src="/images/t-sne.jpg" alt="Scatter plot showing semantic segregation by gender" />
<p>
These scatter plots show the semantic clustering of gender-biased words in podcasts. To investigate their content, I looked at words most representative for each cluster:
</p>
<img src="/images/gender-bias-table.jpg" alt="Table of gender-biased semantic clusters" />
<p>
While it mostly confirmed that podcasts reinforce old stereotypes (women and home/family, men and conflict/sports), I did find one optimistic shift: business and entrepreneurship words, usually male-coded in news data, shifted toward the female cluster in podcasts, reflecting the rise of female creators in the medium.
</p>
</article>
</main>
</body>
</html>